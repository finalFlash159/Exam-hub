[
  {
    "id": 1,
    "question": "What is the correct formula for Gini Information Gain (\\\\( \\Delta \\text{Gini}(A) \\\\)) for an attribute A?",
    "options": [
      {
        "label": "A",
        "text": "\\\\( \\Delta \\text{Gini}(A) = \\text{Gini}_A(D) - \\text{Gini}(D) \\\\)"
      },
      {
        "label": "B",
        "text": "\\\\( \\Delta \\text{Gini}(A) = 1 - \\text{Gini}(D) \\\\)"
      },
      {
        "label": "C",
        "text": "\\\\( \\Delta \\text{Gini}(A) = \\text{Gini}(D) - \\text{Gini}_A(D) \\\\)"
      },
      {
        "label": "D",
        "text": "\\\\( \\Delta \\text{Gini}(A) = \\text{Gini}(D) / \\text{Gini}_A(D) \\\\)"
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Gini Information Gain is \\\\( \\Delta \\text{Gini}(A) = \\text{Gini}(D) - \\sum_{s=1}^{m} \\frac{|D_s|}{|D|} \\text{Gini}(D_s) \\\\).",
      "vi": "Độ lợi thông tin Gini là \\\\( \\Delta \\text{Gini}(A) = \\text{Gini}(D) - \\sum_{s=1}^{m} \\frac{|D_s|}{|D|} \\text{Gini}(D_s) \\\\)."
    }
  },
  {
    "id": 2,
    "question": "When using Gini Information Gain to split a node, which attribute is chosen?",
    "options": [
      {
        "label": "A",
        "text": "The attribute with the lowest Gini Information Gain."
      },
      {
        "label": "B",
        "text": "The attribute with the highest Gini Impurity."
      },
      {
        "label": "C",
        "text": "The attribute that maximizes Gini Information Gain."
      },
      {
        "label": "D",
        "text": "The attribute with the highest sum of child branches' Gini Impurities."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "The attribute with the highest Gini Information Gain is selected, as it reduces impurity the most.",
      "vi": "Thuộc tính có độ lợi thông tin Gini cao nhất được chọn, vì nó giảm độ không thuần khiết nhiều nhất."
    }
  },
  {
    "id": 3,
    "question": "In the material's example, between Outlook (Gini = 0.343) and Wind (Gini = 0.429), which attribute is chosen for splitting and why?",
    "options": [
      {
        "label": "A",
        "text": "Wind, because Gini(Wind) = 0.429 is higher."
      },
      {
        "label": "B",
        "text": "Outlook, because Gini(Outlook) = 0.343 is the lowest."
      },
      {
        "label": "C",
        "text": "Both attributes have equal Gini values."
      },
      {
        "label": "D",
        "text": "Neither is chosen due to high Gini Impurity."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Outlook is chosen because its Gini Impurity (0.343) is lower than Wind's (0.429), indicating a better split.",
      "vi": "Outlook được chọn vì Gini Impurity của nó (0.343) thấp hơn Wind (0.429), biểu thị một điểm chia tốt hơn."
    }
  },
  {
    "id": 4,
    "question": "What type of labels are used in a Regression Tree?",
    "options": [
      {
        "label": "A",
        "text": "Categorical."
      },
      {
        "label": "B",
        "text": "Numeric/continuous values."
      },
      {
        "label": "C",
        "text": "Binary."
      },
      {
        "label": "D",
        "text": "String values."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Regression Trees predict numeric or continuous values, such as temperature or price.",
      "vi": "Cây hồi quy dự đoán giá trị số hoặc liên tục, như nhiệt độ hoặc giá cả."
    }
  },
  {
    "id": 5,
    "question": "What is the main idea behind building a Regression Tree?",
    "options": [
      {
        "label": "A",
        "text": "Find splits to maximize Information Gain."
      },
      {
        "label": "B",
        "text": "Find splits to maximize Gini Impurity."
      },
      {
        "label": "C",
        "text": "Find the best split to minimize Mean Squared Error (MSE)."
      },
      {
        "label": "D",
        "text": "Split data into groups with equal sample sizes."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Regression Trees aim to minimize MSE by finding splits that reduce the variance in predicted values.",
      "vi": "Cây hồi quy nhằm tối thiểu hóa MSE bằng cách tìm các điểm chia giảm phương sai trong giá trị dự đoán."
    }
  },
  {
    "id": 6,
    "question": "What is the first step in finding the best split for an attribute in a Regression Tree?",
    "options": [
      {
        "label": "A",
        "text": "Compute the MSE of the entire dataset."
      },
      {
        "label": "B",
        "text": "Sort the data by the attribute's values."
      },
      {
        "label": "C",
        "text": "Select a random potential split point."
      },
      {
        "label": "D",
        "text": "Split the data into two random subsets."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Sorting the attribute's values allows systematic evaluation of potential split points for minimizing MSE.",
      "vi": "Sắp xếp giá trị của thuộc tính cho phép đánh giá có hệ thống các điểm chia tiềm năng để tối thiểu hóa MSE."
    }
  },
  {
    "id": 7,
    "question": "Which criterion is used to evaluate the quality of a split in a Regression Tree?",
    "options": [
      {
        "label": "A",
        "text": "Entropy."
      },
      {
        "label": "B",
        "text": "Gini Impurity."
      },
      {
        "label": "C",
        "text": "Mean Squared Error (MSE)."
      },
      {
        "label": "D",
        "text": "Information Gain."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "MSE is used to evaluate splits in Regression Trees, measuring the squared error of predictions.",
      "vi": "MSE được sử dụng để đánh giá các điểm chia trong Cây hồi quy, đo lường sai số bình phương của dự đoán."
    }
  },
  {
    "id": 8,
    "question": "After computing MSE for all possible splits (n-1 MSEs), what is the next step to select the split?",
    "options": [
      {
        "label": "A",
        "text": "Choose the split with the highest MSE."
      },
      {
        "label": "B",
        "text": "Choose the split with the lowest MSE."
      },
      {
        "label": "C",
        "text": "Recalculate the training data."
      },
      {
        "label": "D",
        "text": "Stop tree construction and choose a random split."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "The split with the lowest MSE is selected to minimize prediction error in the resulting subsets.",
      "vi": "Điểm chia có MSE thấp nhất được chọn để tối thiểu hóa sai số dự đoán trong các tập con kết quả."
    }
  },
  {
    "id": 9,
    "question": "In a Regression Tree, after splitting data into left and right groups, what is typically the predicted value (\\\\( \\hat{y} \\\\)) for each group?",
    "options": [
      {
        "label": "A",
        "text": "The median value of Y in the group."
      },
      {
        "label": "B",
        "text": "The maximum value of Y in the group."
      },
      {
        "label": "C",
        "text": "The mean value of Y in the group."
      },
      {
        "label": "D",
        "text": "The minimum value of Y in the group."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "The predicted value \\\\( \\hat{y} \\\\) is the mean of Y values in each group, minimizing squared errors.",
      "vi": "Giá trị dự đoán \\\\( \\hat{y} \\\\) là trung bình của các giá trị Y trong mỗi nhóm, tối thiểu hóa sai số bình phương."
    }
  },
  {
    "id": 10,
    "question": "What is the correct formula for the MSE of a split in a Regression Tree, with \\\\( n = n_{\\text{left}} + n_{\\text{right}} \\\\)?",
    "options": [
      {
        "label": "A",
        "text": "\\\\( \\frac{1}{n} \\left( \\sum_{i=1}^{n_{\\text{left}}} (y_i - \\hat{y}_{\\text{left}})^2 - \\sum_{j=1}^{n_{\\text{right}}} (y_j - \\hat{y}_{\\text{right}})^2 \\right) \\\\)"
      },
      {
        "label": "B",
        "text": "\\\\( \\frac{1}{n_{\\text{left}}} \\sum_{i=1}^{n_{\\text{left}}} (y_i - \\hat{y}_{\\text{left}})^2 + \\frac{1}{n_{\\text{right}}} \\sum_{j=1}^{n_{\\text{right}}} (y_j - \\hat{y}_{\\text{right}})^2 \\\\)"
      },
      {
        "label": "C",
        "text": "\\\\( \\sum_{i=1}^{n_{\\text{left}}} (y_i - \\hat{y}_{\\text{left}})^2 + \\sum_{j=1}^{n_{\\text{right}}} (y_j - \\hat{y}_{\\text{right}})^2 \\\\)"
      },
      {
        "label": "D",
        "text": "\\\\( \\frac{1}{n} \\left( \\sum_{i=1}^{n_{\\text{left}}} (y_i - \\hat{y}_{\\text{left}})^2 + \\sum_{j=1}^{n_{\\text{right}}} (y_j - \\hat{y}_{\\text{right}})^2 \\right) \\\\)"
      }
    ],
    "answer": "D",
    "explanation": {
      "en": "MSE is the weighted average of squared errors: \\\\( \\frac{1}{n} \\left( \\sum_{i=1}^{n_{\\text{left}}} (y_i - \\hat{y}_{\\text{left}})^2 + \\sum_{j=1}^{n_{\\text{right}}} (y_j - \\hat{y}_{\\text{right}})^2 \\right) \\\\).",
      "vi": "MSE là trung bình có trọng số của sai số bình phương: \\\\( \\frac{1}{n} \\left( \\sum_{i=1}^{n_{\\text{left}}} (y_i - \\hat{y}_{\\text{left}})^2 + \\sum_{j=1}^{n_{\\text{right}}} (y_j - \\hat{y}_{\\text{right}})^2 \\right) \\\\)."
    }
  },
  {
    "id": 11,
    "question": "What is the purpose of computing Feature Importance in a Decision Tree?",
    "options": [
      {
        "label": "A",
        "text": "To remove irrelevant attributes from the model."
      },
      {
        "label": "B",
        "text": "To optimize training speed."
      },
      {
        "label": "C",
        "text": "To compute a score representing each attribute's importance to the model's predictions."
      },
      {
        "label": "D",
        "text": "To determine the maximum depth of the tree."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Feature Importance quantifies each attribute's contribution to predictions, aiding in feature selection and interpretation.",
      "vi": "Tầm quan trọng của thuộc tính định lượng mức độ đóng góp của mỗi thuộc tính vào dự đoán, hỗ trợ chọn lọc và diễn giải."
    }
  },
  {
    "id": 12,
    "question": "In the Feature Importance formula for a single Decision Tree, what does \\\\( p(n) \\\\) represent?",
    "options": [
      {
        "label": "A",
        "text": "The purity of node \\\\( n \\\\)."
      },
      {
        "label": "B",
        "text": "The probability a sample passes through node \\\\( n \\\\) using attribute \\\\( i \\\\)."
      },
      {
        "label": "C",
        "text": "The number of child nodes of node \\\\( n \\\\)."
      },
      {
        "label": "D",
        "text": "The total number of nodes in the tree."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "\\\\( p(n) \\\\) is the probability that a sample reaches node \\\\( n \\\\), reflecting its importance in splits using attribute \\\\( i \\\\).",
      "vi": "\\\\( p(n) \\\\) là xác suất một mẫu đi qua nút \\\\( n \\\\), phản ánh tầm quan trọng của nó trong các điểm chia sử dụng thuộc tính \\\\( i \\\\)."
    }
  },
  {
    "id": 13,
    "question": "What metrics can measure the 'purity of a node' in the Feature Importance formula?",
    "options": [
      {
        "label": "A",
        "text": "MSE and RMSE."
      },
      {
        "label": "B",
        "text": "Accuracy and Recall."
      },
      {
        "label": "C",
        "text": "Entropy, Gini, or Squared Error."
      },
      {
        "label": "D",
        "text": "Information Gain and Gini Information Gain."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Node purity can be measured using Entropy, Gini Impurity, or Squared Error, depending on the task (classification or regression).",
      "vi": "Độ tinh khiết của nút có thể được đo bằng Entropy, Gini Impurity, hoặc Sai số Bình phương, tùy thuộc vào nhiệm vụ (phân loại hoặc hồi quy)."
    }
  },
  {
    "id": 14,
    "question": "How is Feature Importance for an attribute in a single Decision Tree calculated according to the material?",
    "options": [
      {
        "label": "A",
        "text": "Sum of Gini Impurities of all nodes using that attribute."
      },
      {
        "label": "B",
        "text": "Sum of Entropy of all nodes using that attribute."
      },
      {
        "label": "C",
        "text": "Sum of purity reductions caused by that attribute across all nodes."
      },
      {
        "label": "D",
        "text": "Number of times the attribute is used in splits."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Feature Importance is the sum of reductions in impurity (e.g., Gini or Entropy) caused by the attribute across all nodes.",
      "vi": "Tầm quan trọng của thuộc tính là tổng sự giảm độ không thuần khiết (ví dụ: Gini hoặc Entropy) do thuộc tính gây ra trên tất cả các nút."
    }
  },
  {
    "id": 15,
    "question": "In a Random Forest, how is the importance of an attribute calculated?",
    "options": [
      {
        "label": "A",
        "text": "Importance of the attribute in the first tree only."
      },
      {
        "label": "B",
        "text": "Average importance of the attribute across all trees."
      },
      {
        "label": "C",
        "text": "Sum of the attribute's importance across all trees."
      },
      {
        "label": "D",
        "text": "Highest importance of the attribute in any single tree."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Feature Importance in a Random Forest is the average of the attribute's importance across all trees in the forest.",
      "vi": "Tầm quan trọng của thuộc tính trong Rừng ngẫu nhiên là trung bình của tầm quan trọng của thuộc tính trên tất cả các cây trong rừng."
    }
  },
  {
    "id": 16,
    "question": "In the Random Forest Feature Importance formula \\\\( I_i = \\frac{1}{|B|} \\sum_{T \\in B} I_i(T) \\\\), what does \\\\( |B| \\\\) represent?",
    "options": [
      {
        "label": "A",
        "text": "The number of attributes in the model."
      },
      {
        "label": "B",
        "text": "The total number of nodes across all trees."
      },
      {
        "label": "C",
        "text": "The total number of trees in the Random Forest."
      },
      {
        "label": "D",
        "text": "The number of training samples."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "\\\\( |B| \\\\) is the total number of trees in the Random Forest, used to average the feature importance.",
      "vi": "\\\\( |B| \\\\) là tổng số cây trong Rừng ngẫu nhiên, dùng để tính trung bình tầm quan trọng của thuộc tính."
    }
  },
  {
    "id": 17,
    "question": "In the Random Forest Feature Importance formula \\\\( I_i = \\frac{1}{|B|} \\sum_{T \\in B} I_i(T) \\\\), what does \\\\( I_i(T) \\\\) represent?",
    "options": [
      {
        "label": "A",
        "text": "The importance of attribute \\\\( i \\\\) across the entire forest."
      },
      {
        "label": "B",
        "text": "The total importance of all attributes in tree \\\\( T \\\\)."
      },
      {
        "label": "C",
        "text": "The importance of attribute \\\\( i \\\\) for a single tree \\\\( T \\\\)."
      },
      {
        "label": "D",
        "text": "The Gini Impurity of tree \\\\( T \\\\)."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "\\\\( I_i(T) \\\\) is the importance of attribute \\\\( i \\\\) in a single tree \\\\( T \\\\), summed and averaged across all trees.",
      "vi": "\\\\( I_i(T) \\\\) là tầm quan trọng của thuộc tính \\\\( i \\\\) trong một cây đơn lẻ \\\\( T \\\\), được tổng hợp và lấy trung bình trên tất cả các cây."
    }
  },
  {
    "id": 18,
    "question": "What does a high Feature Importance score for an attribute imply for the model?",
    "options": [
      {
        "label": "A",
        "text": "The attribute is less relevant to the model's predictions."
      },
      {
        "label": "B",
        "text": "The attribute has a greater influence on the model's predictions."
      },
      {
        "label": "C",
        "text": "The attribute is used only in leaf nodes."
      },
      {
        "label": "D",
        "text": "The attribute causes overfitting in the model."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "A high Feature Importance score indicates the attribute significantly influences the model's predictions.",
      "vi": "Điểm Tầm quan trọng của thuộc tính cao biểu thị thuộc tính có ảnh hưởng lớn đến dự đoán của mô hình."
    }
  },
  {
    "id": 19,
    "question": "How is Feature Importance normalized (sum to 1) in the example using Entropy or Gini?",
    "options": [
      {
        "label": "A",
        "text": "Divide each score by the number of attributes."
      },
      {
        "label": "B",
        "text": "Take the square root of the sum of scores."
      },
      {
        "label": "C",
        "text": "Divide each score by the sum of all computed importance scores."
      },
      {
        "label": "D",
        "text": "Multiply by a random coefficient."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Normalization divides each score by the sum of all importance scores to ensure they sum to 1.",
      "vi": "Chuẩn hóa chia mỗi điểm cho tổng của tất cả các điểm tầm quan trọng để đảm bảo tổng bằng 1."
    }
  },
  {
    "id": 20,
    "question": "In the Gini-based Feature Importance example, if the raw scores are 0.1670 for 'satisfaction_level' and 0.1440 for 'time_spend_company,' what is the normalized score for 'satisfaction_level'?",
    "options": [
      {
        "label": "A",
        "text": "\\\\( 0.1670 \\times (0.1670 + 0.1440) \\\\)"
      },
      {
        "label": "B",
        "text": "\\\\( \\frac{0.1440}{0.1670 + 0.1440} \\\\)"
      },
      {
        "label": "C",
        "text": "\\\\( \\frac{0.1670}{0.1670 + 0.1440} \\approx 0.537 \\\\)"
      },
      {
        "label": "D",
        "text": "\\\\( 0.1670 - 0.1440 \\\\)"
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Normalized score = \\\\( \\frac{0.1670}{0.1670 + 0.1440} = \\frac{0.1670}{0.3110} \\approx 0.537 \\\\).",
      "vi": "Điểm chuẩn hóa = \\\\( \\frac{0.1670}{0.1670 + 0.1440} = \\frac{0.1670}{0.3110} \\approx 0.537 \\\\)."
    }
  },
  {
    "id": 21,
    "category": "Terminology",
    "question": "What is the purpose of handling missing values in a Decision Tree algorithm?",
    "options": [
      {
        "label": "A",
        "text": "To increase the depth of the tree."
      },
      {
        "label": "B",
        "text": "To ensure accurate splits and improve model robustness."
      },
      {
        "label": "C",
        "text": "To reduce the number of attributes in the dataset."
      },
      {
        "label": "D",
        "text": "To simplify the computation of Entropy."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Handling missing values ensures that the Decision Tree can make accurate splits by imputing or managing incomplete data, improving model robustness.",
      "vi": "Xử lý giá trị thiếu đảm bảo Cây Quyết định có thể thực hiện các điểm chia chính xác bằng cách nội suy hoặc quản lý dữ liệu không đầy đủ, cải thiện độ bền của mô hình."
    }
  },
  {
    "id": 22,
    "category": "Terminology",
    "question": "Which technique is commonly used to handle missing values in Decision Trees?",
    "options": [
      {
        "label": "A",
        "text": "Deleting all samples with missing values."
      },
      {
        "label": "B",
        "text": "Using surrogate splits to assign samples to branches."
      },
      {
        "label": "C",
        "text": "Randomly assigning missing values to a class."
      },
      {
        "label": "D",
        "text": "Increasing the tree depth to compensate."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Surrogate splits use alternative attributes to approximate the best split when data is missing, maintaining tree accuracy.",
      "vi": "Điểm chia thay thế sử dụng các thuộc tính thay thế để xấp xỉ điểm chia tốt nhất khi dữ liệu bị thiếu, duy trì độ chính xác của cây."
    }
  },
  {
    "id": 23,
    "category": "Problem-Solving Scenarios",
    "question": "If a Decision Tree model overfits due to excessive depth, which action is most effective to mitigate this?",
    "options": [
      {
        "label": "A",
        "text": "Increase the number of attributes used for splitting."
      },
      {
        "label": "B",
        "text": "Set a maximum depth constraint during training."
      },
      {
        "label": "C",
        "text": "Use only continuous attributes for splits."
      },
      {
        "label": "D",
        "text": "Train the model on a smaller dataset."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Setting a maximum depth constraint limits tree complexity, reducing overfitting by preventing overly specific splits.",
      "vi": "Đặt giới hạn độ sâu tối đa làm giảm độ phức tạp của cây, giảm quá khớp bằng cách ngăn chặn các điểm chia quá cụ thể."
    }
  },
  {
    "id": 24,
    "category": "Problem-Solving Scenarios",
    "question": "In a Random Forest, how does the 'max_features' hyperparameter affect model performance?",
    "options": [
      {
        "label": "A",
        "text": "It determines the maximum depth of each tree."
      },
      {
        "label": "B",
        "text": "It limits the number of trees in the forest."
      },
      {
        "label": "C",
        "text": "It controls the number of features considered for each split."
      },
      {
        "label": "D",
        "text": "It sets the minimum number of samples per leaf."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "'max_features' controls the number of features randomly selected for each split, promoting diversity and reducing overfitting.",
      "vi": "'max_features' kiểm soát số lượng thuộc tính được chọn ngẫu nhiên cho mỗi điểm chia, thúc đẩy sự đa dạng và giảm quá khớp."
    }
  },
  {
    "id": 25,
    "category": "Problem-Solving Scenarios",
    "question": "When training a Random Forest on an imbalanced dataset, what technique can improve classification performance?",
    "options": [
      {
        "label": "A",
        "text": "Use a single Decision Tree instead."
      },
      {
        "label": "B",
        "text": "Apply class weighting or oversampling during training."
      },
      {
        "label": "C",
        "text": "Increase the number of features in the dataset."
      },
      {
        "label": "D",
        "text": "Reduce the number of trees in the forest."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Class weighting or oversampling balances the influence of minority classes, improving Random Forest performance on imbalanced data.",
      "vi": "Cân bằng lớp hoặc lấy mẫu quá mức làm cân bằng ảnh hưởng của các lớp thiểu số, cải thiện hiệu suất Rừng ngẫu nhiên trên dữ liệu không cân bằng."
    }
  },
  {
    "id": 26,
    "category": "Calculations",
    "question": "Given a node with 10 samples (6 class A, 4 class B), what is the Gini Impurity?",
    "options": [
      {
        "label": "A",
        "text": "\\\\( 1 - \\left( \\left( \\frac{6}{10} \\right)^2 + \\left( \\frac{4}{10} \\right)^2 \\right) \\approx 0.48 \\\\)"
      },
      {
        "label": "B",
        "text": "\\\\( \\frac{6}{10} \\log_2\\left( \\frac{6}{10} \\right) + \\frac{4}{10} \\log_2\\left( \\frac{4}{10} \\right) \\approx 0.97 \\\\)"
      },
      {
        "label": "C",
        "text": "\\\\( 1 - \\left( \\frac{6}{10} + \\frac{4}{10} \\right) = 0 \\\\)"
      },
      {
        "label": "D",
        "text": "\\\\( \\left( \\frac{6}{10} \\right)^2 + \\left( \\frac{4}{10} \\right)^2 \\approx 0.52 \\\\)"
      }
    ],
    "answer": "A",
    "explanation": {
      "en": "Gini Impurity = \\\\( 1 - \\left( \\left( \\frac{6}{10} \\right)^2 + \\left( \\frac{4}{10} \\right)^2 \\right) = 1 - (0.36 + 0.16) = 0.48 \\\\).",
      "vi": "Gini Impurity = \\\\( 1 - \\left( \\left( \\frac{6}{10} \\right)^2 + \\left( \\frac{4}{10} \\right)^2 \\right) = 1 - (0.36 + 0.16) = 0.48 \\\\)."
    }
  },
  {
    "id": 27,
    "category": "Calculations",
    "question": "For a dataset with Entropy \\\\( H(S) = 0.971 \\\\), splitting on attribute A yields subsets with Entropies 0.811 and 0.918, weighted by \\(\\frac{5}{10}\\) and \\(\\frac{5}{10}\\). What is the Information Gain?",
    "options": [
      {
        "label": "A",
        "text": "\\\\( 0.971 - (0.811 + 0.918) \\approx -0.758 \\\\)"
      },
      {
        "label": "B",
        "text": "\\\\( 0.971 - \\left( \\frac{5}{10} \\cdot 0.811 + \\frac{5}{10} \\cdot 0.918 \\right) \\approx 0.107 \\\\)"
      },
      {
        "label": "C",
        "text": "\\\\( \\frac{5}{10} \\cdot 0.811 + \\frac{5}{10} \\cdot 0.918 \\approx 0.864 \\\\)"
      },
      {
        "label": "D",
        "text": "\\\\( 0.971 + (0.811 - 0.918) \\approx 0.864 \\\\)"
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Information Gain = \\\\( H(S) - \\sum \\frac{|S_i|}{|S|} H(S_i) = 0.971 - \\left( \\frac{5}{10} \\cdot 0.811 + \\frac{5}{10} \\cdot 0.918 \\right) = 0.971 - 0.8645 \\approx 0.107 \\\\).",
      "vi": "Độ lợi thông tin = \\\\( H(S) - \\sum \\frac{|S_i|}{|S|} H(S_i) = 0.971 - \\left( \\frac{5}{10} \\cdot 0.811 + \\frac{5}{10} \\cdot 0.918 \\right) = 0.971 - 0.8645 \\approx 0.107 \\\\)."
    }
  },
  {
    "id": 28,
    "category": "Calculations",
    "question": "In a Regression Tree, a split divides 8 samples into left (4 samples, Y=[2, 3, 4, 5]) and right (4 samples, Y=[6, 7, 8, 9]). What is the MSE of the split?",
    "options": [
      {
        "label": "A",
        "text": "\\\\( \\frac{1}{8} \\left( (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 \\right) \\approx 1.25 \\\\)"
      },
      {
        "label": "B",
        "text": "\\\\( \\frac{1}{8} \\left( \\sum_{i=1}^{4} (y_i - 3.5)^2 + \\sum_{j=1}^{4} (y_j - 7.5)^2 \\right) \\approx 2.5 \\\\)"
      },
      {
        "label": "C",
        "text": "\\\\( \\sum_{i=1}^{4} (y_i - 3.5)^2 + \\sum_{j=1}^{4} (y_j - 7.5)^2 = 20 \\\\)"
      },
      {
        "label": "D",
        "text": "\\\\( \\frac{1}{4} \\left( (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 \\right) \\approx 2.5 \\\\)"
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Left mean = 3.5, Right mean = 7.5. MSE = \\\\( \\frac{1}{8} \\left( (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-7.5)^2 + (7-7.5)^2 + (8-7.5)^2 + (9-7.5)^2 \\right) = \\frac{20}{8} = 2.5 \\\\).",
      "vi": "Trung bình trái = 3.5, trung bình phải = 7.5. MSE = \\\\( \\frac{1}{8} \\left( (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-7.5)^2 + (7-7.5)^2 + (8-7.5)^2 + (9-7.5)^2 \\right) = \\frac{20}{8} = 2.5 \\\\)."
    }
  },
  {
    "id": 29,
    "category": "Problem-Solving Scenarios",
    "question": "When tuning a Random Forest, increasing the number of trees (K) typically has what effect?",
    "options": [
      {
        "label": "A",
        "text": "Increases overfitting risk."
      },
      {
        "label": "B",
        "text": "Reduces variance and stabilizes predictions."
      },
      {
        "label": "C",
        "text": "Decreases the model's accuracy."
      },
      {
        "label": "D",
        "text": "Reduces the feature importance scores."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Increasing K in a Random Forest reduces variance by averaging more trees, leading to more stable predictions.",
      "vi": "Tăng K trong Rừng ngẫu nhiên làm giảm phương sai bằng cách lấy trung bình nhiều cây hơn, dẫn đến dự đoán ổn định hơn."
    }
  },
  {
    "id": 30,
    "category": "Terminology",
    "question": "What is 'pre-pruning' in the context of Decision Trees?",
    "options": [
      {
        "label": "A",
        "text": "Removing branches after the tree is fully grown."
      },
      {
        "label": "B",
        "text": "Stopping tree growth early based on predefined criteria."
      },
      {
        "label": "C",
        "text": "Randomly selecting attributes to split."
      },
      {
        "label": "D",
        "text": "Combining multiple trees into a forest."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Pre-pruning stops tree growth early using criteria like minimum impurity decrease or maximum depth to prevent overfitting.",
      "vi": "Tỉa trước dừng phát triển cây sớm dựa trên các tiêu chí như giảm độ không thuần khiết tối thiểu hoặc độ sâu tối đa để ngăn quá khớp."
    }
  },
  {
    "question": "What is the main advantage of the Gini impurity over entropy for decision tree splitting?",
    "options": [
      {
        "label": "A",
        "text": "Gini is computationally faster as it doesn't require logarithm calculations"
      },
      {
        "label": "B",
        "text": "Gini always produces better accuracy"
      },
      {
        "label": "C",
        "text": "Gini handles missing values better"
      },
      {
        "label": "D",
        "text": "Gini prevents overfitting"
      }
    ],
    "answer": "A",
    "explanation": {
      "en": "Gini impurity is computationally more efficient than entropy because it doesn't involve logarithmic calculations.",
      "vi": "Độ tạp Gini hiệu quả hơn về mặt tính toán so với entropy vì không cần tính toán logarit."
    },
    "id": 31
  },
  {
    "question": "In Random Forest, what does the 'bootstrap aggregating' (bagging) technique specifically do?",
    "options": [
      {
        "label": "A",
        "text": "Creates multiple training sets by sampling with replacement"
      },
      {
        "label": "B",
        "text": "Selects the best features automatically"
      },
      {
        "label": "C",
        "text": "Prunes individual trees"
      },
      {
        "label": "D",
        "text": "Calculates feature importance"
      }
    ],
    "answer": "A",
    "explanation": {
      "en": "Bootstrap aggregating creates multiple training datasets by sampling with replacement from the original training set.",
      "vi": "Tổng hợp bootstrap tạo nhiều tập dữ liệu huấn luyện bằng cách lấy mẫu có hoàn lại từ tập huấn luyện gốc."
    },
    "id": 32
  },
  {
    "question": "What is the primary purpose of pruning in decision trees?",
    "options": [
      {
        "label": "A",
        "text": "To reduce overfitting and improve generalization"
      },
      {
        "label": "B",
        "text": "To increase the tree depth"
      },
      {
        "label": "C",
        "text": "To speed up training"
      },
      {
        "label": "D",
        "text": "To increase accuracy on training data"
      }
    ],
    "answer": "A",
    "explanation": {
      "en": "Pruning removes unnecessary branches to prevent overfitting and improve the model's ability to generalize.",
      "vi": "Cắt tỉa loại bỏ các nhánh không cần thiết để ngăn chặn overfitting và cải thiện khả năng tổng quát hóa của mô hình."
    },
    "id": 33
  },
  {
    "question": "What is the difference between pre-pruning and post-pruning?",
    "options": [
      {
        "label": "A",
        "text": "Pre-pruning stops growth early, post-pruning removes branches after full growth"
      },
      {
        "label": "B",
        "text": "Pre-pruning is more accurate than post-pruning"
      },
      {
        "label": "C",
        "text": "They are the same technique"
      },
      {
        "label": "D",
        "text": "Pre-pruning is only for regression trees"
      }
    ],
    "answer": "A",
    "explanation": {
      "en": "Pre-pruning stops tree growth based on stopping criteria, while post-pruning grows the full tree then removes branches.",
      "vi": "Cắt tỉa trước dừng phát triển cây dựa trên tiêu chí dừng, trong khi cắt tỉa sau phát triển cây đầy đủ rồi loại bỏ các nhánh."
    },
    "id": 34
  },
  {
    "question": "In Random Forest, what is the typical value for the number of features to consider at each split (mtry)?",
    "options": [
      {
        "label": "A",
        "text": "All features"
      },
      {
        "label": "B",
        "text": "Square root of total features for classification"
      },
      {
        "label": "C",
        "text": "Half of total features"
      },
      {
        "label": "D",
        "text": "Always 5 features"
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "For classification, mtry is typically set to the square root of the total number of features.",
      "vi": "Đối với phân loại, mtry thường được đặt bằng căn bậc hai của tổng số đặc trưng."
    },
    "id": 35
  }
]