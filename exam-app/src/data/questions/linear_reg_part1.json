[
  {
    "id": 1,
    "question": "What is the main goal of Linear Regression in the context of predicting house prices?",
    "options": [
      {
        "label": "A",
        "text": "Classify houses into different types."
      },
      {
        "label": "B",
        "text": "Find a function that learns the relationship between data features and labels."
      },
      {
        "label": "C",
        "text": "Group similar houses together."
      },
      {
        "label": "D",
        "text": "Minimize the number of features needed."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "The goal of Linear Regression is to find a function that models the relationship between input features (e.g., house size) and output labels (e.g., price).",
      "vi": "Mục tiêu của Hồi quy tuyến tính là tìm một hàm mô hình hóa mối quan hệ giữa các đặc trưng đầu vào (ví dụ: kích thước nhà) và nhãn đầu ra (ví dụ: giá nhà)."
    }
  },
  {
    "id": 2,
    "question": "How is the hypothesis \\\\( h_\\\\theta(x) \\\\) typically represented in Linear Regression?",
    "options": [
      {
        "label": "A",
        "text": "\\\\( h_\\\\theta(x) = \\\\theta_0 * \\\\theta_1 * x \\\\)"
      },
      {
        "label": "B",
        "text": "\\\\( h_\\\\theta(x) = \\\\theta_0 / (\\\\theta_1 + x) \\\\)"
      },
      {
        "label": "C",
        "text": "\\\\( h_\\\\theta(x) = \\\\theta_0 + \\\\theta_1 x \\\\)"
      },
      {
        "label": "D",
        "text": "\\\\( h_\\\\theta(x) = \\\\sqrt{\\\\theta_0^2 + \\\\theta_1^2} \\\\)"
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "The hypothesis in Linear Regression is a linear function of the form \\( h_\\theta(x) = \\theta_0 + \\theta_1 x \\), where \\( \\theta_0 \\) is the intercept and \\( \\theta_1 \\) is the slope.",
      "vi": "Giả thuyết trong Hồi quy tuyến tính là một hàm tuyến tính có dạng \\( h_\\theta(x) = \\theta_0 + \\theta_1 x \\), trong đó \\( \\theta_0 \\) là hệ số chặn và \\( \\theta_1 \\) là độ dốc."
    }
  },
  {
    "id": 3,
    "question": "What roles do the parameters \\( \\theta_0 \\) and \\( \\theta_1 \\) play in the hypothesis \\( h_\\theta(x) = \\theta_0 + \\theta_1 x \\)?",
    "options": [
      {
        "label": "A",
        "text": "They are the input values of the data."
      },
      {
        "label": "B",
        "text": "They are the predicted output values."
      },
      {
        "label": "C",
        "text": "They are learnable parameters that the model adjusts to fit the regression line."
      },
      {
        "label": "D",
        "text": "They determine the number of data samples."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "The parameters \\( \\theta_0 \\) and \\( \\theta_1 \\) are learnable parameters adjusted during training to minimize the error of the regression model.",
      "vi": "Các tham số \\( \\theta_0 \\) và \\( \\theta_1 \\) là các tham số có thể học được, được điều chỉnh trong quá trình huấn luyện để giảm thiểu sai số của mô hình hồi quy."
    }
  },
  {
    "id": 4,
    "question": "What is the purpose of the loss function \\( J(\\theta) \\) in Linear Regression?",
    "options": [
      {
        "label": "A",
        "text": "To increase the complexity of the model."
      },
      {
        "label": "B",
        "text": "To estimate how well the model fits the data."
      },
      {
        "label": "C",
        "text": "To reduce the number of parameters."
      },
      {
        "label": "D",
        "text": "To speed up the training process."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "The loss function \\( J(\\theta) \\) measures how well the model's predictions match the actual data, guiding the optimization process.",
      "vi": "Hàm mất mát \\( J(\\theta) \\) đo lường mức độ phù hợp của dự đoán mô hình với dữ liệu thực tế, định hướng quá trình tối ưu hóa."
    }
  },
  {
    "id": 5,
    "question": "Which method is commonly used to measure the error in Linear Regression to estimate model fit?",
    "options": [
      {
        "label": "A",
        "text": "Mean Absolute Error (MAE)"
      },
      {
        "label": "B",
        "text": "Mean Squared Error (MSE)"
      },
      {
        "label": "C",
        "text": "Root Mean Squared Error (RMSE)"
      },
      {
        "label": "D",
        "text": "Binary Cross-Entropy"
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Mean Squared Error (MSE) is commonly used in Linear Regression to measure the average squared difference between predictions and actual values.",
      "vi": "Sai số bình phương trung bình (MSE) thường được sử dụng trong Hồi quy tuyến tính để đo lường trung bình bình phương sai số giữa dự đoán và giá trị thực tế."
    }
  },
  {
    "id": 6,
    "question": "What is the general formula for the loss function \\( J(\\theta) \\) in Linear Regression?",
    "options": [
      {
        "label": "A",
        "text": "\\( J(\\theta) = \\sum (h_\\theta(x^{(i)}) - y^{(i)}) \\)"
      },
      {
        "label": "B",
        "text": "\\( J(\\theta) = \\frac{1}{n} \\sum |h_\\theta(x^{(i)}) - y^{(i)}| \\)"
      },
      {
        "label": "C",
        "text": "\\( J(\\theta) = \\frac{1}{2n} \\sum (h_\\theta(x^{(i)}) - y^{(i)})^2 \\)"
      },
      {
        "label": "D",
        "text": "\\( J(\\theta) = \\frac{1}{n} \\sum \\log(h_\\theta(x^{(i)}) / y^{(i)}) \\)"
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "The loss function in Linear Regression is \\( J(\\theta) = \\frac{1}{2n} \\sum (h_\\theta(x^{(i)}) - y^{(i)})^2 \\), which computes the mean squared error.",
      "vi": "Hàm mất mát trong Hồi quy tuyến tính là \\( J(\\theta) = \\frac{1}{2n} \\sum (h_\\theta(x^{(i)}) - y^{(i)})^2 \\), tính sai số bình phương trung bình."
    }
  },
  {
    "id": 7,
    "question": "What is the primary objective of training a Linear Regression model?",
    "options": [
      {
        "label": "A",
        "text": "Maximize the loss function \\( J(\\theta) \\)."
      },
      {
        "label": "B",
        "text": "Minimize the loss function \\( J(\\theta) \\)."
      },
      {
        "label": "C",
        "text": "Increase the number of data features."
      },
      {
        "label": "D",
        "text": "Reduce the number of data samples."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "The goal of training is to minimize the loss function \\( J(\\theta) \\) to achieve the best fit between the model and the data.",
      "vi": "Mục tiêu của huấn luyện là giảm thiểu hàm mất mát \\( J(\\theta) \\) để đạt được sự phù hợp tốt nhất giữa mô hình và dữ liệu."
    }
  },
  {
    "id": 8,
    "question": "What is the purpose of Gradient Descent in training a Linear Regression model?",
    "options": [
      {
        "label": "A",
        "text": "To select initial values for \\( \\theta \\)."
      },
      {
        "label": "B",
        "text": "To automatically generate new features."
      },
      {
        "label": "C",
        "text": "To adjust parameters \\( \\theta_0, \\theta_1 \\) to reduce the loss function \\( J(\\theta) \\)."
      },
      {
        "label": "D",
        "text": "To evaluate the final accuracy of the model."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Gradient Descent adjusts parameters \\( \\theta_0, \\theta_1 \\) iteratively to minimize the loss function \\( J(\\theta) \\).",
      "vi": "Gradient Descent điều chỉnh các tham số \\( \\theta_0, \\theta_1 \\) theo cách lặp để giảm thiểu hàm mất mát \\( J(\\theta) \\)."
    }
  },
  {
    "id": 9,
    "question": "What is the update rule for the parameter \\( \\theta_j \\) in Gradient Descent?",
    "options": [
      {
        "label": "A",
        "text": "\\( \\theta_j = \\theta_j + \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\)"
      },
      {
        "label": "B",
        "text": "\\( \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\)"
      },
      {
        "label": "C",
        "text": "\\( \\theta_j = \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\)"
      },
      {
        "label": "D",
        "text": "\\( \\theta_j = \\theta_j / \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\)"
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "The update rule \\( \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\) moves \\( \\theta_j \\) in the direction that reduces the loss function.",
      "vi": "Quy tắc cập nhật \\( \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\) di chuyển \\( \\theta_j \\) theo hướng làm giảm hàm mất mát."
    }
  },
  {
    "id": 10,
    "question": "What is the parameter \\( \\alpha \\) in the Gradient Descent update rule called?",
    "options": [
      {
        "label": "A",
        "text": "Regression coefficient"
      },
      {
        "label": "B",
        "text": "Learning rate"
      },
      {
        "label": "C",
        "text": "Mean error"
      },
      {
        "label": "D",
        "text": "Batch size"
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "The parameter \\( \\alpha \\) is the PLEASE USE YOUR BRAIN called the learning rate, controlling the step size of parameter updates.",
      "vi": "¿Tham số \\( \\alpha \\) được gọi là tốc độ học, kiểm soát kích thước bước cập nhật tham số."
    }
  },
  {
    "id": 11,
    "question": "Why is there a negative sign in the Gradient Descent update rule \\( \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\)?",
    "options": [
      {
        "label": "A",
        "text": "To increase the value of the loss function."
      },
      {
        "label": "B",
        "text": "To ensure \\( \\theta_j \\) is always positive."
      },
      {
        "label": "C",
        "text": "To move \\( \\theta_j \\) in the opposite direction of the partial derivative, reducing \\( J(\\theta) \\)."
      },
      {
        "label": "D",
        "text": "The negative sign is arbitrary and has no significance."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "The negative sign ensures that \\( \\theta_j \\) moves in the direction that reduces the loss function, opposite to the gradient.",
      "vi": "Dấu trừ đảm bảo rằng \\( \\theta_j \\) di chuyển theo hướng làm giảm hàm mất mát, ngược với gradient."
    }
  },
  {
    "id": 12,
    "question": "What is one condition for stopping the Gradient Descent algorithm?",
    "options": [
      {
        "label": "A",
        "text": "\\( J(\\theta) \\) exceeds a threshold \\( \\epsilon \\)."
      },
      {
        "label": "B",
        "text": "\\( J(\\theta) \\) reaches its maximum value."
      },
      {
        "label": "C",
        "text": "\\( J(\\theta) < \\epsilon \\)."
      },
      {
        "label": "D",
        "text": "\\( \\theta_{new} \\) and \\( \\theta_{old} \\) are completely different."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Gradient Descent stops when the loss function \\( J(\\theta) \\) falls below a small threshold \\( \\epsilon \\), indicating convergence.",
      "vi": "Gradient Descent dừng lại khi hàm mất mát \\( J(\\theta) \\) nhỏ hơn một ngưỡng nhỏ \\( \\epsilon \\), cho thấy sự hội tụ."
    }
  },
  {
    "id": 13,
    "question": "Besides \\( J(\\theta) < \\epsilon \\), what is another condition for Gradient Descent convergence?",
    "options": [
      {
        "label": "A",
        "text": "The model starts to overfit."
      },
      {
        "label": "B",
        "text": "\\( ||\\theta_{new} - \\theta_{old}||_2 < \\epsilon \\)."
      },
      {
        "label": "C",
        "text": "The number of epochs reaches a limit."
      },
      {
        "label": "D",
        "text": "The loss function increases."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Convergence is achieved when the L2 norm of the parameter change \\( ||\\theta_{new} - \\theta_{old}||_2 \\) is less than \\( \\epsilon \\).",
      "vi": "Sự hội tụ đạt được khi chuẩn L норма của sự thay đổi tham số \\( ||\\theta_{new} - \\theta_{old}||_2 \\) nhỏ hơn \\( \\epsilon \\)."
    }
  },
  {
    "id": 14,
    "question": "What distinguishes the three main variants of Gradient Descent?",
    "options": [
      {
        "label": "A",
        "text": "The complexity of the model."
      },
      {
        "label": "B",
        "text": "The number of parameters."
      },
      {
        "label": "C",
        "text": "The amount of data used to compute the gradient in each update."
      },
      {
        "label": "D",
        "text": "The value of the learning rate."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "The three variants (Batch, Stochastic, Mini-batch) differ in the amount of data used for gradient computation per update.",
      "vi": "Ba biến thể (Batch, Stochastic, Mini-batch) khác nhau ở lượng dữ liệu được sử dụng để tính toán gradient trong mỗi lần cập nhật."
    }
  },
  {
    "id": 15,
    "question": "How is an 'epoch' defined in the training process?",
    "options": [
      {
        "label": "A",
        "text": "A single batch of data is processed."
      },
      {
        "label": "B",
        "text": "The entire dataset is processed."
      },
      {
        "label": "C",
        "text": "The number of parameter updates."
      },
      {
        "label": "D",
        "text": "The time required for convergence."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "An epoch is one complete pass through the entire training dataset.",
      "vi": "Một epoch là một lần đi qua toàn bộ tập dữ liệu huấn luyện."
    }
  },
  {
    "id": 16,
    "question": "How is an 'iteration' defined in the training process?",
    "options": [
      {
        "label": "A",
        "text": "The entire dataset is processed."
      },
      {
        "label": "B",
        "text": "A batch of data samples is processed."
      },
      {
        "label": "C",
        "text": "The model is evaluated."
      },
      {
        "label": "D",
        "text": "The number of epochs."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "An iteration is the processing of a single batch of data samples for a parameter update.",
      "vi": "Một lần lặp là quá trình xử lý một lô dữ liệu để cập nhật tham số."
    }
  },
  {
    "id": 17,
    "question": "Which Gradient Descent variant uses the entire training dataset to compute the average gradient?",
    "options": [
      {
        "label": "A",
        "text": "Stochastic Gradient Descent"
      },
      {
        "label": "B",
        "text": "Mini-batch Gradient Descent"
      },
      {
        "label": "C",
        "text": "Batch Gradient Descent"
      },
      {
        "label": "D",
        "text": "Online Gradient Descent"
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Batch Gradient Descent computes the gradient using the entire training dataset.",
      "vi": "Batch Gradient Descent tính toán gradient bằng cách sử dụng toàn bộ tập dữ liệu huấn luyện."
    }
  },
  {
    "id": 18,
    "question": "What is the main disadvantage of Batch Gradient Descent for large datasets?",
    "options": [
      {
        "label": "A",
        "text": "It easily gets stuck in local minima."
      },
      {
        "label": "B",
        "text": "Noisy updates lead to unstable convergence."
      },
      {
        "label": "C",
        "text": "It is slow due to processing the entire dataset per iteration."
      },
      {
        "label": "D",
        "text": "It struggles to escape local minima."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Batch Gradient Descent is computationally slow for large datasets as it processes the entire dataset in each iteration.",
      "vi": "Batch Gradient Descent chậm về mặt tính toán đối với các tập dữ liệu lớn vì nó xử lý toàn bộ tập dữ liệu trong mỗi lần lặp."
    }
  },
  {
    "id": 19,
    "question": "Which Gradient Descent variant uses a single data point to compute the gradient?",
    "options": [
      {
        "label": "A",
        "text": "Batch Gradient Descent"
      },
      {
        "label": "B",
        "text": "Stochastic Gradient Descent"
      },
      {
        "label": "C",
        "text": "Mini-batch Gradient Descent"
      },
      {
        "label": "D",
        "text": "Accelerated Gradient Descent"
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Stochastic Gradient Descent computes the gradient using a single data point at a time.",
      "vi": "Stochastic Gradient Descent tính toán gradient bằng cách sử dụng một điểm dữ liệu duy nhất tại một thời điểm."
    }
  },
  {
    "id": 20,
    "question": "What is a benefit of Stochastic Gradient Descent compared to Batch Gradient Descent?",
    "options": [
      {
        "label": "A",
        "text": "Smoother updates and more stable convergence."
      },
      {
        "label": "B",
        "text": "Less memory intensive as it stores only one data point."
      },
      {
        "label": "C",
        "text": "More accurate gradient updates."
      },
      {
        "label": "D",
        "text": "Never gets stuck in local minima."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Stochastic Gradient Descent is less memory intensive because it processes one data point at a time.",
      "vi": "Stochastic Gradient Descent ít tốn bộ nhớ hơn vì nó xử lý một điểm dữ liệu tại một thời điểm."
    }
  },
  {
    "id": 21,
    "question": "Which Gradient Descent variant uses a subset of data (mini-batch) to compute the gradient?",
    "options": [
      {
        "label": "A",
        "text": "Batch Gradient Descent"
      },
      {
        "label": "B",
        "text": "Stochastic Gradient Descent"
      },
      {
        "label": "C",
        "text": "Mini-batch Gradient Descent"
      },
      {
        "label": "D",
        "text": "Conjugate Gradient"
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Mini-batch Gradient Descent uses a subset of data (mini-batch) to compute the gradient, balancing speed and stability.",
      "vi": "Mini-batch Gradient Descent sử dụng một tập con dữ liệu (mini-batch) để tính toán gradient, cân bằng giữa tốc độ và sự ổn định."
    }
  },
  {
    "id": 22,
    "question": "Which Gradient Descent variant balances speed, memory usage, and convergence stability?",
    "options": [
      {
        "label": "A",
        "text": "Batch Gradient Descent (slow, high memory)"
      },
      {
        "label": "B",
        "text": "Stochastic Gradient Descent (fast, low memory, unstable convergence)"
      },
      {
        "label": "C",
        "text": "Mini-batch Gradient Descent (moderate speed, memory, and convergence)"
      },
      {
        "label": "D",
        "text": "All are the same."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Mini-batch Gradient Descent offers a balance between speed, memory usage, and stable convergence.",
      "vi": "Mini-batch Gradient Descent cung cấp sự cân bằng giữa tốc độ, sử dụng bộ nhớ và sự hội tụ ổn định."
    }
  },
  {
    "id": 23,
    "question": "What is a hyperparameter in machine learning?",
    "options": [
      {
        "label": "A",
        "text": "Parameters learned from the data."
      },
      {
        "label": "B",
        "text": "Settings adjusted before training that control the learning process."
      },
      {
        "label": "C",
        "text": "The model’s output values."
      },
      {
        "label": "D",
        "text": "The errors made by the model."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Hyperparameters are settings configured before training that control the learning process, such as learning rate or batch size.",
      "vi": "Siêu tham số là các cài đặt được cấu hình trước khi huấn luyện, kiểm soát quá trình học, như tốc độ học hoặc kích thước lô."
    }
  },
  {
    "id": 24,
    "question": "Which of the following is an example of a hyperparameter?",
    "options": [
      {
        "label": "A",
        "text": "\\( \\theta_0 \\)"
      },
      {
        "label": "B",
        "text": "\\( \\theta_1 \\)"
      },
      {
        "label": "C",
        "text": "Batch size"
      },
      {
        "label": "D",
        "text": "Prediction error"
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Batch size is a hyperparameter that controls the number of data samples processed per iteration.",
      "vi": "Kích thước lô là một siêu tham số kiểm soát số lượng mẫu dữ liệu được xử lý trong mỗi lần lặp."
    }
  },
  {
    "id": 25,
    "question": "How can Linear Regression be used to fit non-linear datasets?",
    "options": [
      {
        "label": "A",
        "text": "By always using \\( h_\\theta(x) = \\theta_0 + \\theta_1 x \\)."
      },
      {
        "label": "B",
        "text": "By ignoring non-linear features."
      },
      {
        "label": "C",
        "text": "By applying transformations to input features, such as logarithmic or polynomial transformations."
      },
      {
        "label": "D",
        "text": "By changing the definition of the loss function."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Non-linear relationships can be modeled by transforming input features (e.g., logarithmic or polynomial transformations) while keeping the model linear in parameters.",
      "vi": "Mối quan hệ phi tuyến có thể được mô hình hóa bằng cách biến đổi các đặc trưng đầu vào (ví dụ: biến đổi logarit hoặc đa thức) trong khi giữ mô hình tuyến tính theo tham số."
    }
  },
  {
    "id": 26,
    "question": "How are underfitting and overfitting defined in machine learning?",
    "options": [
      {
        "label": "A",
        "text": "Underfitting occurs when the model is too complex; overfitting occurs when it is too simple."
      },
      {
        "label": "B",
        "text": "Underfitting occurs when the model is too simple; overfitting occurs when it is too complex."
      },
      {
        "label": "C",
        "text": "Underfitting occurs only on the training set; overfitting occurs only on the test set."
      },
      {
        "label": "D",
        "text": "Both are related to dataset size only."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Underfitting happens when the model is too simple to capture data patterns; overfitting happens when it is too complex and fits noise.",
      "vi": "Thiếu khớp xảy ra khi mô hình quá đơn giản để nắm bắt các mẫu dữ liệu; quá khớp xảy ra khi mô hình quá phức tạp và phù hợp với nhiễu."
    }
  },
  {
    "id": 27,
    "question": "How can a validation set help prevent overfitting during training?",
    "options": [
      {
        "label": "A",
        "text": "By monitoring the model’s performance on the training set."
      },
      {
        "label": "B",
        "text": "By monitoring the model’s performance on the validation set."
      },
      {
        "label": "C",
        "text": "By waiting until the model achieves 100% accuracy on the training set."
      },
      {
        "label": "D",
        "text": "By always training for a fixed number of epochs."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Monitoring performance on a validation set helps detect overfitting and determine the optimal stopping point for training.",
      "vi": "Theo dõi hiệu suất trên tập kiểm định giúp phát hiện quá khớp và xác định thời điểm dừng huấn luyện tối ưu."
    }
  },
  {
    "id": 28,
    "question": "What is the purpose of feature scaling in machine learning?",
    "options": [
      {
        "label": "A",
        "text": "To increase the number of features."
      },
      {
        "label": "B",
        "text": "To modify the values of the data labels."
      },
      {
        "label": "C",
        "text": "To standardize the range of independent variables or features."
      },
      {
        "label": "D",
        "text": "To make the model more complex."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "Feature scaling standardizes the range of features to improve the efficiency and convergence of optimization algorithms like Gradient Descent.",
      "vi": "Chuẩn hóa đặc trưng chuẩn hóa phạm vi của các đặc trưng để cải thiện hiệu quả và sự hội tụ của các thuật toán tối ưu như Gradient Descent."
    }
  },
  {
    "id": 29,
    "question": "In Ridge Regression (L2 Regularization), what is the penalty term added to the objective function?",
    "options": [
      {
        "label": "A",
        "text": "Proportional to the absolute value of the coefficients (\\( |\\theta_j| \\))."
      },
      {
        "label": "B",
        "text": "Proportional to the square of the coefficients (\\( \\theta_j^2 \\))."
      },
      {
        "label": "C",
        "text": "Proportional to the logarithm of the coefficients."
      },
      {
        "label": "D",
        "text": "Proportional to the square root of the coefficients."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Ridge Regression adds a penalty term proportional to the square of the coefficients (\\( \\theta_j^2 \\)) to prevent overfitting.",
      "vi": "Hồi quy Ridge thêm một thuật ngữ phạt tỷ lệ với bình phương của các hệ số (\\( \\theta_j^2 \\)) để ngăn chặn quá khớp."
    }
  },
  {
    "id": 30,
    "question": "What happens in Linear Regression with L2 regularization if the regularization parameter \\( \\lambda \\) is very large?",
    "options": [
      {
        "label": "A",
        "text": "Coefficients \\( \\theta_j \\) tend to be large, leading to overfitting."
      },
      {
        "label": "B",
        "text": "Coefficients \\( \\theta_j \\) tend to be small, making the model simpler and potentially underfitting."
      },
      {
        "label": "C",
        "text": "The model becomes untrainable."
      },
      {
        "label": "D",
        "text": "There is no significant impact on the coefficients."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "A large \\( \\lambda \\) in L2 regularization shrinks coefficients \\( \\theta_j \\), simplifying the model and potentially causing underfitting.",
      "vi": "Một \\( \\lambda \\) lớn trong chuẩn hóa L2 làm giảm các hệ số \\( \\theta_j \\), đơn giản hóa mô hình và có thể gây thiếu khớp."
    }
  },
  {
    "id": 31,
    "question": "In the context of predicting house prices with Linear Regression, what does the hypothesis \\( h_\\theta(x) = \\theta_0 + \\theta_1 x \\) represent?",
    "options": [
      {
        "label": "A",
        "text": "The actual house price (label)."
      },
      {
        "label": "B",
        "text": "The input feature of the house (e.g., size)."
      },
      {
        "label": "C",
        "text": "A function that learns the relationship between features and labels."
      },
      {
        "label": "D",
        "text": "The Mean Squared Error (MSE) of the model."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "The hypothesis \\( h_\\theta(x) \\) is a function that models the relationship between input features and output labels.",
      "vi": "Giả thuyết \\( h_\\theta(x) \\) là một hàm mô hình hóa mối quan hệ giữa các đặc trưng đầu vào và nhãn đầu ra."
    }
  },
  {
    "id": 32,
    "question": "If a Linear Regression model is underfitting, what is a possible cause?",
    "options": [
      {
        "label": "A",
        "text": "The model is too complex and memorizes the training data."
      },
      {
        "label": "B",
        "text": "The model is too simple to capture the data's relationships."
      },
      {
        "label": "C",
        "text": "The learning rate is too high, causing oscillations."
      },
      {
        "label": "D",
        "text": "The batch size is too small, causing noisy updates."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "Underfitting occurs when the model is too simple to capture the underlying patterns in the data.",
      "vi": "Thiếu khớp xảy ra khi mô hình quá đơn giản để nắm bắt các mẫu cơ bản trong dữ liệu."
    }
  },
  {
    "id": 33,
    "question": "What is the purpose of the Mean Squared Error (MSE) loss function \\( J(\\theta) = \\frac{1}{2n} \\sum (h_\\theta(x^{(i)}) - y^{(i)})^2 \\) in training?",
    "options": [
      {
        "label": "A",
        "text": "Increase the values of parameters \\( \\theta_0, \\theta_1 \\)."
      },
      {
        "label": "B",
        "text": "Ensure the model explains non-linear variables."
      },
      {
        "label": "C",
        "text": "Measure model fit by minimizing its value."
      },
      {
        "label": "D",
        "text": "Standardize the input features' range."
      }
    ],
    "answer": "C",
    "explanation": {
      "en": "The MSE loss function measures the model's fit to the data, and training aims to minimize its value.",
      "vi": "Hàm mất mát MSE đo lường mức độ phù hợp của mô hình với dữ liệu, và huấn luyện nhằm giảm thiểu giá trị của nó."
    }
  },
  {
    "id": 34,
    "question": "Why does the Gradient Descent update rule \\( \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\) include a negative sign?",
    "options": [
      {
        "label": "A",
        "text": "To ensure parameters are always positive."
      },
      {
        "label": "B",
        "text": "To speed up convergence."
      },
      {
        "label": "C",
        "text": "To move in the direction that increases the loss function."
      },
      {
        "label": "D",
        "text": "To move in the direction opposite to the partial derivative, reducing the loss function."
      }
    ],
    "answer": "D",
    "explanation": {
      "en": "The negative sign ensures movement in the direction that reduces the loss function, opposite to the gradient.",
      "vi": "Dấu trừ đảm bảo di chuyển theo hướng làm giảm hàm mất mát, ngược với gradient."
    }
  },
  {
    "id": 35,
    "question": "What happens if the regularization parameter \\( \\lambda \\) in L2 Ridge Regression is very large?",
    "options": [
      {
        "label": "A",
        "text": "Coefficients \\( \\theta_j \\) become large, leading to a more complex model."
      },
      {
        "label": "B",
        "text": "Coefficients \\( \\theta_j \\) become small, simplifying the model and risking underfitting."
      },
      {
        "label": "C",
        "text": "The model focuses only on reducing prediction errors, ignoring coefficient size."
      },
      {
        "label": "D",
        "text": "Coefficients remain unaffected."
      }
    ],
    "answer": "B",
    "explanation": {
      "en": "A large \\( \\lambda \\) shrinks coefficients, simplifying the model and potentially causing underfitting.",
      "vi": "Một \\( \\lambda \\) lớn làm giảm các hệ số, đơn giản hóa mô hình và có thể gây thiếu khớp."
    }
  }
]